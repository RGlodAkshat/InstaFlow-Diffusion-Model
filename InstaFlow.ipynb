{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "pkgs = [\n",
    "    \"torchmetrics==1.4.0.post0\",   # FID + CLIPScore\n",
    "    \"torchvision\",                 # only used for optional fallback real images\n",
    "    \"ftfy\", \"regex\", \"tqdm\"        # CLIP/tokenizer deps & nice progress\n",
    "]\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *pkgs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplatform\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sys\u001b[38;5;241m.\u001b[39mexecutable)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m__version__, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| CUDA?\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "File \u001b[1;32mc:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\lib\\site-packages\\diffusers\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.36.0.dev0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     DIFFUSERS_SLOW_IMPORT,\n\u001b[0;32m      7\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m      8\u001b[0m     _LazyModule,\n\u001b[0;32m      9\u001b[0m     is_accelerate_available,\n\u001b[0;32m     10\u001b[0m     is_bitsandbytes_available,\n\u001b[0;32m     11\u001b[0m     is_flax_available,\n\u001b[0;32m     12\u001b[0m     is_gguf_available,\n\u001b[0;32m     13\u001b[0m     is_k_diffusion_available,\n\u001b[0;32m     14\u001b[0m     is_librosa_available,\n\u001b[0;32m     15\u001b[0m     is_note_seq_available,\n\u001b[0;32m     16\u001b[0m     is_nvidia_modelopt_available,\n\u001b[0;32m     17\u001b[0m     is_onnx_available,\n\u001b[0;32m     18\u001b[0m     is_opencv_available,\n\u001b[0;32m     19\u001b[0m     is_optimum_quanto_available,\n\u001b[0;32m     20\u001b[0m     is_scipy_available,\n\u001b[0;32m     21\u001b[0m     is_sentencepiece_available,\n\u001b[0;32m     22\u001b[0m     is_torch_available,\n\u001b[0;32m     23\u001b[0m     is_torchao_available,\n\u001b[0;32m     24\u001b[0m     is_torchsde_available,\n\u001b[0;32m     25\u001b[0m     is_transformers_available,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Lazy Import based on\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# https://github.com/huggingface/transformers/blob/main/src/transformers/__init__.py\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# When adding a new object to this init, please add it to `_import_structure`. The `_import_structure` is a dictionary submodule to list of object names,\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# and is used to defer the actual importing for when the objects are requested.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# This way `import diffusers` provides the names in the namespace without actually importing anything (and especially none of the backends).\u001b[39;00m\n\u001b[0;32m     36\u001b[0m _import_structure \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration_utils\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfigMixin\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mguiders\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     ],\n\u001b[0;32m     65\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\lib\\site-packages\\diffusers\\utils\\__init__.py:128\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_logger\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseOutput\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    129\u001b[0m     check_peft_version,\n\u001b[0;32m    130\u001b[0m     delete_adapter_layers,\n\u001b[0;32m    131\u001b[0m     get_adapter_name,\n\u001b[0;32m    132\u001b[0m     get_peft_kwargs,\n\u001b[0;32m    133\u001b[0m     recurse_remove_peft_layers,\n\u001b[0;32m    134\u001b[0m     scale_lora_layers,\n\u001b[0;32m    135\u001b[0m     set_adapter_layers,\n\u001b[0;32m    136\u001b[0m     set_weights_and_activate_adapters,\n\u001b[0;32m    137\u001b[0m     unscale_lora_layers,\n\u001b[0;32m    138\u001b[0m )\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpil_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PIL_INTERPOLATION, make_image_grid, numpy_to_pil, pt_to_pil\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mremote_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m remote_decode\n",
      "File \u001b[1;32mc:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\lib\\site-packages\\diffusers\\utils\\peft_utils.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_peft_available, is_peft_version, is_torch_available\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m empty_device_cache\n\u001b[0;32m     29\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n",
      "File \u001b[1;32mc:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\lib\\site-packages\\diffusers\\utils\\torch_utils.py:83\u001b[0m\n\u001b[0;32m     80\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m allow_in_graph \u001b[38;5;28;01mas\u001b[39;00m maybe_allow_in_graph\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m):\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmaybe_allow_in_graph\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\lib\\site-packages\\torch\\_dynamo\\__init__.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, convert_frame, eval_frame, resume_execution\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[1;32mc:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CallbackTrigger\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "File \u001b[1;32mc:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:58\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m guard_bool\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_method\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     59\u001b[0m     config,\n\u001b[0;32m     60\u001b[0m     exc,\n\u001b[0;32m     61\u001b[0m     graph_break_hints,\n\u001b[0;32m     62\u001b[0m     logging \u001b[38;5;28;01mas\u001b[39;00m torchdynamo_logging,\n\u001b[0;32m     63\u001b[0m     trace_rules,\n\u001b[0;32m     64\u001b[0m     variables,\n\u001b[0;32m     65\u001b[0m )\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_analysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     67\u001b[0m     get_indexof,\n\u001b[0;32m     68\u001b[0m     JUMP_OPNAMES,\n\u001b[0;32m     69\u001b[0m     livevars_analysis,\n\u001b[0;32m     70\u001b[0m     propagate_line_nums,\n\u001b[0;32m     71\u001b[0m )\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     73\u001b[0m     cleaned_instructions,\n\u001b[0;32m     74\u001b[0m     create_call_function,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m     unique_id,\n\u001b[0;32m     82\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\lib\\site-packages\\torch\\_dynamo\\trace_rules.py:47\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, cast, Optional, Union\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest_operators\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_content_store\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\lib\\site-packages\\torch\\_inductor\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, IO, Literal, Optional, TYPE_CHECKING, Union\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstandalone_compile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompiledArtifact  \u001b[38;5;66;03m# noqa: TC001\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\lib\\site-packages\\torch\\_inductor\\config.py:533\u001b[0m\n\u001b[0;32m    528\u001b[0m autoheuristic_log_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTORCHINDUCTOR_AUTOHEURISTIC_LOG_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEFAULT\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    530\u001b[0m )\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Disabled by default on ROCm, opt-in if model utilises NHWC convolutions\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m layout_opt_default \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[38;5;241m.\u001b[39mhip \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m layout_optimization \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    535\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTORCHINDUCTOR_LAYOUT_OPTIMIZATION\u001b[39m\u001b[38;5;124m\"\u001b[39m, layout_opt_default) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    536\u001b[0m )\n\u001b[0;32m    538\u001b[0m force_layout_optimization \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTORCHINDUCTOR_FORCE_LAYOUT_OPT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\lib\\site-packages\\torch\\__init__.py:2745\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m   2742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[0;32m   2743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m-> 2745\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "import sys, platform, torch, diffusers\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA?\", torch.cuda.is_available())\n",
    "print(\"Diffusers:\", diffusers.__version__)\n",
    "\n",
    "# ==== InstaFlow explorer + one-layer W4 quantization (single cell) ==========================\n",
    "import os, json, csv, time, math, argparse, sys\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "# %% CLIPScore eval (TorchMetrics-safe): per-image display + CSV\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchmetrics.multimodal import CLIPScore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, torch\n",
    "from PIL import Image\n",
    "from Rectified import RectifiedFlowPipeline\n",
    "\n",
    "# Try native RectifiedFlowPipeline; fall back to custom pipeline if needed\n",
    "pipe = RectifiedFlowPipeline.from_pretrained(\n",
    "    \"XCLiu/instaflow_0_9B_from_sd_1_5\",\n",
    "    torch_dtype=torch.float32,\n",
    "    use_safetensors=True,\n",
    ").to(\"cpu\")\n",
    "print(\"Using native RectifiedFlowPipeline\")\n",
    "    \n",
    "# Keep CPU happy\n",
    "torch.set_num_threads(max(1, os.cpu_count() - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Montain\"\n",
    "W = H = 512      # start smaller on CPU; bump later if needed\n",
    "t0 = time.time()\n",
    "img = pipe(\n",
    "    prompt=prompt,\n",
    "    width=W, height=H,\n",
    "    num_inference_steps=1,      # InstaFlow is one-step\n",
    "    guidance_scale=0.0\n",
    ").images[0]\n",
    "out = \"Output_Images/image.png\"\n",
    "img.save(out)\n",
    "print(f\"Saved {out} in {time.time()-t0:.2f}s (size {W}x{H}, steps=1, guidance=0.0)\")\n",
    "Image.open(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, psutil\n",
    "meta = {\n",
    "    \"prompt\": prompt ,\n",
    "    \"resolution\": f\"{W}x{H}\",\n",
    "    \"steps\": 1,\n",
    "    \"guidance\": 0.0,\n",
    "    \"cpu_logical_cores\": psutil.cpu_count(logical=True),\n",
    "    \"python\": platform.python_version(),\n",
    "    \"torch\": torch.__version__,\n",
    "    \"diffusers\": __import__(\"diffusers\").__version__,\n",
    "}\n",
    "print(json.dumps(meta, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Of Baseline Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, torch\n",
    "from PIL import Image\n",
    "\n",
    "def generate_and_time(pipe, prompt, width=512, height=512, runs=3):\n",
    "    times = []\n",
    "    outs = []\n",
    "    torch.set_num_threads(max(1, os.cpu_count()-1))\n",
    "    for i in range(runs):\n",
    "        t0 = time.perf_counter()\n",
    "        img = pipe(prompt=prompt,\n",
    "                   width=width, height=height,\n",
    "                   num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "        dt = time.perf_counter() - t0\n",
    "        img.save(f\"Output_Images/instaflow_{width}x{height}_{i+1}.png\")\n",
    "        times.append(dt); outs.append(img)\n",
    "        print(f\"[{i+1}/{runs}] {width}x{height} in {dt:.2f}s -> instaflow_{width}x{height}_{i+1}.png\")\n",
    "    print(f\"\\nAvg time: {sum(times)/len(times):.2f}s | Min: {min(times):.2f}s | Max: {max(times):.2f}s\")\n",
    "    return times, outs\n",
    "\n",
    "# Example:\n",
    "_= generate_and_time(pipe, prompt=\"Cute cat, studio lighting\", width=512, height=512, runs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP Score Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% CLIPScore eval (manual CLIP preprocessing; per-image scores + CSV + display)\n",
    "import os, csv\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML\n",
    "import torch\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Prompts\n",
    "simple_prompts = [\n",
    "    \"a cute dog\",\n",
    "    \"a beautiful mountain\",\n",
    "    \"a bowl of fruit\",\n",
    "    \"a happy cat\",\n",
    "    \"a sunset over the ocean\",\n",
    "    \"a man riding a bike\",\n",
    "    \"a delicious pizza\",\n",
    "    \"a child playing football\",\n",
    "    \"a red sports car\",\n",
    "    \"a flower in the rain\"\n",
    "]\n",
    "\n",
    "# 1) Generate images with your existing `pipe`\n",
    "os.makedirs(\"Output_Images\", exist_ok=True)\n",
    "W, H = 512, 512\n",
    "gen_paths, gen_pils = [], []\n",
    "for i, p in enumerate(simple_prompts, 1):\n",
    "    im = pipe(prompt=p, width=W, height=H, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "    path = f\"Output_Images/clip_{i:02d}.png\"\n",
    "    im.save(path)\n",
    "    gen_paths.append(path)\n",
    "    gen_pils.append(Image.open(path).convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Convert PIL -> **uint8 tensors** [C,H,W] (no normalization, no /255)\n",
    "imgs_tensor_list = [TF.pil_to_tensor(img) for img in gen_pils]  # 0–255 uint8\n",
    "\n",
    "# 3) Build CLIPScore metric and compute per-image scores\n",
    "metric = CLIPScore(model_name_or_path=\"openai/clip-vit-large-patch14\").to(\"cpu\")\n",
    "\n",
    "scores = []\n",
    "with torch.no_grad():\n",
    "    for img_t, prompt in zip(imgs_tensor_list, simple_prompts):\n",
    "        # Pass single-item lists (image list, text list)\n",
    "        s = metric([img_t], [prompt]).item()\n",
    "        scores.append(s)\n",
    "\n",
    "avg = sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Display each image with its CLIPScore\n",
    "display(HTML(\"<h3>CLIPScore Results (Option A)</h3>\"))\n",
    "for idx, (p, path, s) in enumerate(zip(simple_prompts, gen_paths, scores), 1):\n",
    "    display(HTML(f\"<b>{idx:02d}. Prompt:</b> {p}<br><b>CLIPScore:</b> {s:.3f}\"))\n",
    "    display(Image.open(path))\n",
    "\n",
    "# 5) Save CSV file with per-image and average scores\n",
    "csv_path = \"baseline_clip_score.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"index\", \"image_path\", \"prompt\", \"clip_score\"])\n",
    "    for i, (path, p, s) in enumerate(zip(gen_paths, simple_prompts, scores), 1):\n",
    "        writer.writerow([i, path, p, f\"{s:.6f}\"])\n",
    "    writer.writerow([])\n",
    "    writer.writerow([\"avg\", \"\", \"\", f\"{avg:.6f}\"])\n",
    "\n",
    "print(f\"\\nSaved per-image CLIP scores and average to: {csv_path}\")\n",
    "print(f\"Average CLIPScore over {len(simple_prompts)} prompts: {avg:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- utilities -----------------------------\n",
    "def human(n: int) -> str:\n",
    "    if n >= 1_000_000_000: return f\"{n/1_000_000_000:.2f}B\"\n",
    "    if n >= 1_000_000:     return f\"{n/1_000_000:.2f}M\"\n",
    "    if n >= 1_000:         return f\"{n/1_000:.2f}K\"\n",
    "    return str(n)\n",
    "\n",
    "def count_params(m: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "\n",
    "def shape_of(w: torch.Tensor) -> str:\n",
    "    return \"x\".join(str(int(s)) for s in w.shape)\n",
    "\n",
    "def print_header(title: str):\n",
    "    print(\"\\n\" + \"=\" * 88)\n",
    "    print(title)\n",
    "    print(\"=\" * 88)\n",
    "\n",
    "def print_table(rows: List[List[str]], headers: List[str], max_rows: int = 60):\n",
    "    if not rows:\n",
    "        print(\"(no rows)\")\n",
    "        return\n",
    "    rows = rows[:max_rows]\n",
    "    widths = [max(len(str(x)) for x in col) for col in zip(headers, *rows)]\n",
    "    line = \" | \".join(h.ljust(w) for h, w in zip(headers, widths))\n",
    "    sep  = \"-+-\".join(\"-\" * w for w in widths)\n",
    "    print(line)\n",
    "    print(sep)\n",
    "    for r in rows:\n",
    "        print(\" | \".join(str(c).ljust(w) for c, w in zip(r, widths)))\n",
    "    if len(rows) == max_rows:\n",
    "        print(f\"... (showing first {max_rows} rows)\")\n",
    "\n",
    "def is_probably_fragile(name: str, mod: nn.Module) -> bool:\n",
    "    # Sensible first-pass skip-list\n",
    "    if \"conv_in\" in name or \"conv_out\" in name: return True\n",
    "    if \"time_embedding\" in name: return True\n",
    "    if isinstance(mod, (nn.LayerNorm, nn.GroupNorm)): return True\n",
    "    if \"attentions\" in name and (\"to_out\" in name or \"proj_out\" in name): return True\n",
    "    return False\n",
    "\n",
    "def list_unet_dense_layers(unet: nn.Module) -> List[Tuple[str, nn.Module]]:\n",
    "    layers = []\n",
    "    for name, mod in unet.named_modules():\n",
    "        if isinstance(mod, (nn.Conv2d, nn.Linear)):\n",
    "            layers.append((name, mod))\n",
    "    return layers\n",
    "\n",
    "# ----------------------------- explore components -----------------------------\n",
    "print_header(\"Top-level components (parameter counts)\")\n",
    "components = {\n",
    "    \"text_encoder\": pipe.text_encoder,\n",
    "    \"unet\"        : pipe.unet,\n",
    "    \"vae\"         : pipe.vae,\n",
    "    \"safety_checker\": pipe.safety_checker,\n",
    "}\n",
    "rows = []\n",
    "total = 0\n",
    "for k, m in components.items():\n",
    "    n = count_params(m) if isinstance(m, nn.Module) else 0\n",
    "    rows.append([k, human(n)])\n",
    "    total += n\n",
    "rows.append([\"TOTAL (nn.Modules)\", human(total)])\n",
    "print_table(rows, headers=[\"Component\", \"Params\"])\n",
    "\n",
    "# ----------------------------- UNet layer inventory -----------------------------\n",
    "print_header(\"UNet Conv/Linear inventory\")\n",
    "layers = list_unet_dense_layers(pipe.unet)\n",
    "\n",
    "inv_rows = []\n",
    "csv_rows  = []\n",
    "json_items = []\n",
    "for idx, (name, mod) in enumerate(layers):\n",
    "    w = mod.weight\n",
    "    wshape = shape_of(w)\n",
    "    has_bias = (mod.bias is not None)\n",
    "    n_params = w.numel() + (mod.bias.numel() if has_bias else 0)\n",
    "    note = \"SKIP?\" if is_probably_fragile(name, mod) else \"\"\n",
    "    inv_rows.append([idx, name, type(mod).__name__, wshape, \"Y\" if has_bias else \"N\", human(n_params), note])\n",
    "    csv_rows.append([idx, name, type(mod).__name__, wshape, has_bias, n_params, note])\n",
    "    json_items.append({\n",
    "        \"idx\": idx, \"name\": name, \"type\": type(mod).__name__,\n",
    "        \"weight_shape\": wshape, \"has_bias\": has_bias,\n",
    "        \"param_count\": int(n_params),\n",
    "        \"suggest_skip\": bool(note)\n",
    "    })\n",
    "\n",
    "print_table(inv_rows, headers=[\"#\", \"name\", \"type\", \"weight_shape\", \"bias\", \"params\", \"note\"])\n",
    "\n",
    "# Save inventory\n",
    "os.makedirs(\"Inventory\", exist_ok=True)\n",
    "csv_path  = \"Inventory/unet_layers.csv\"\n",
    "json_path = \"Inventory/unet_layers.json\"\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"idx\",\"name\",\"type\",\"weight_shape\",\"has_bias\",\"param_count\",\"note\"])\n",
    "    writer.writerows(csv_rows)\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump({\"layers\": json_items}, f, indent=2)\n",
    "print(f\"\\n[info] Saved inventory:\\n - {csv_path}\\n - {json_path}\")\n",
    "\n",
    "# ----------------------------- pick a single target layer -----------------------------\n",
    "# Option A: choose by index after reviewing the printed table / CSV\n",
    "TARGET_IDX = next((i for i,(n, m) in enumerate(layers) if (\"down_blocks.0.resnets.0.conv1\" in n)), 0)\n",
    "tgt_name, tgt_mod = layers[TARGET_IDX]\n",
    "print_header(\"Target layer for 4-bit (weights-only) test\")\n",
    "print(\"Index :\", TARGET_IDX)\n",
    "print(\"Name  :\", tgt_name)\n",
    "print(\"Type  :\", type(tgt_mod).__name__)\n",
    "print(\"Shape :\", shape_of(tgt_mod.weight))\n",
    "\n",
    "# ----------------------------- tiny W4 quantizer (weights-only) -----------------------------\n",
    "def per_channel_int4_quantize_weight(W: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Per-output-channel symmetric INT4 PTQ:\n",
    "      s_c = max(|W_c|)/7 ; q = round(W/s_c) clipped to [-8,7] stored as int8 (values in 4-bit domain).\n",
    "    \"\"\"\n",
    "    oc = W.shape[0]\n",
    "    Wc = W.detach().clone().view(oc, -1)\n",
    "    scales = Wc.abs().max(dim=1).values / 7.0\n",
    "    scales = torch.clamp(scales, min=1e-8)\n",
    "    q = torch.round((Wc.T / scales).T)\n",
    "    q = torch.clamp(q, -8, 7).to(torch.int8)  # store as int8; values are 4-bit domain\n",
    "    return q, scales\n",
    "\n",
    "def dequantize_int4(q: torch.Tensor, scales: torch.Tensor, shape):\n",
    "    oc = shape[0]\n",
    "    Wf = (q.float().T * scales).T\n",
    "    return Wf.view(shape)\n",
    "\n",
    "class QConv2d_OneLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop-in wrapper for a single Conv2d that:\n",
    "      - stores INT4 weights (+ per-channel scales)\n",
    "      - dequantizes to float each forward (quality study; not for speed)\n",
    "    \"\"\"\n",
    "    def __init__(self, conv: nn.Conv2d):\n",
    "        super().__init__()\n",
    "        # meta\n",
    "        self.stride, self.padding = conv.stride, conv.padding\n",
    "        self.dilation, self.groups = conv.dilation, conv.groups\n",
    "        self.bias = conv.bias is not None\n",
    "        if self.bias:\n",
    "            self.register_buffer(\"biasv\", conv.bias.data.clone())\n",
    "\n",
    "        # quantize once\n",
    "        q, s = per_channel_int4_quantize_weight(conv.weight.data)\n",
    "        self.register_buffer(\"qweight\", q)\n",
    "        self.register_buffer(\"scales\", s)\n",
    "\n",
    "        self.out_channels = conv.out_channels\n",
    "        self.in_channels  = conv.in_channels\n",
    "        self.kernel_size  = conv.kernel_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        W = dequantize_int4(\n",
    "            self.qweight, self.scales,\n",
    "            (self.out_channels, self.in_channels, *self.kernel_size)\n",
    "        )\n",
    "        return nn.functional.conv2d(\n",
    "            x, W, bias=(self.biasv if self.bias else None),\n",
    "            stride=self.stride, padding=self.padding,\n",
    "            dilation=self.dilation, groups=self.groups\n",
    "        )\n",
    "\n",
    "class QLinear_OneLayer(nn.Module):\n",
    "    def __init__(self, lin: nn.Linear):\n",
    "        super().__init__()\n",
    "        # treat Linear as Conv1x1 for quant/dequant convenience\n",
    "        w4d = lin.weight.data.unsqueeze(-1).unsqueeze(-1)  # (out,in,1,1)\n",
    "        q, s = per_channel_int4_quantize_weight(w4d)\n",
    "        self.register_buffer(\"qweight\", q)\n",
    "        self.register_buffer(\"scales\", s)\n",
    "        self.bias = lin.bias is not None\n",
    "        if self.bias:\n",
    "            self.register_buffer(\"biasv\", lin.bias.data.clone())\n",
    "        self.out_features = lin.out_features\n",
    "        self.in_features  = lin.in_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        W = dequantize_int4(self.qweight, self.scales, (self.out_features, self.in_features, 1, 1))\n",
    "        W = W.squeeze(-1).squeeze(-1)  # (out,in)\n",
    "        y = x @ W.t()\n",
    "        return y + (self.biasv if self.bias else 0.0)\n",
    "\n",
    "# ----------------------------- replace just one layer -----------------------------\n",
    "# Walk to the parent module of the target, then swap it\n",
    "parent = pipe.unet\n",
    "parts = tgt_name.split(\".\")\n",
    "for p in parts[:-1]:\n",
    "    parent = getattr(parent, p)\n",
    "leaf = parts[-1]\n",
    "\n",
    "orig_mod = getattr(parent, leaf)\n",
    "if isinstance(orig_mod, nn.Conv2d):\n",
    "    setattr(parent, leaf, QConv2d_OneLayer(orig_mod))\n",
    "    replaced_type = \"Conv2d -> QConv2d_OneLayer\"\n",
    "elif isinstance(orig_mod, nn.Linear):\n",
    "    setattr(parent, leaf, QLinear_OneLayer(orig_mod))\n",
    "    replaced_type = \"Linear -> QLinear_OneLayer\"\n",
    "else:\n",
    "    replaced_type = f\"(no-op) unsupported type: {type(orig_mod).__name__}\"\n",
    "\n",
    "print(f\"[info] Replaced: {tgt_name}   ({replaced_type})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Bit Quantization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [single-cell] InstaFlow 8-bit (weights-only) PTQ demo — load → quantize → (pipe_8bitQ)\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# What this cell does:\n",
    "#   1) Loads a fresh InstaFlow pipeline into `pipe_8bitQ` (keeps your baseline untouched)\n",
    "#   2) Finds UNet Conv/Linear layers\n",
    "#   3) Applies simple **INT8 weights-only** per-output-channel PTQ\n",
    "#      - symmetric quantization to [-127, +127]\n",
    "#      - fragile layers (conv_in/conv_out/norms/attn out-proj) are skipped on first pass\n",
    "#   4) Leaves activations in float (no runtime speedup yet; quality exploration first)\n",
    "#\n",
    "# Notes:\n",
    "#   • If MODEL_ID / DEVICE / TORCH_DTYPE / INV_DIR / SAVE_INVENTORY already exist from previous cells, we reuse them.\n",
    "#     Otherwise, we set safe defaults below.\n",
    "#   • This creates a separate pipeline named `pipe_8bitQ`.\n",
    "# -----------------------------------------------------------------------------------------\n",
    "\n",
    "import os, json, csv, time\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# ---------- Fallback CONFIG (reuse if already defined) ----------\n",
    "try:\n",
    "    MODEL_ID\n",
    "except NameError:\n",
    "    MODEL_ID = \"XCLiu/instaflow_0_9B_from_sd_1_5\"\n",
    "try:\n",
    "    DEVICE\n",
    "except NameError:\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "try:\n",
    "    TORCH_DTYPE\n",
    "except NameError:\n",
    "    TORCH_DTYPE = torch.float16 if (DEVICE == \"cuda\") else torch.float32\n",
    "try:\n",
    "    INV_DIR\n",
    "except NameError:\n",
    "    INV_DIR = \"Inventory\"\n",
    "try:\n",
    "    SAVE_INVENTORY\n",
    "except NameError:\n",
    "    SAVE_INVENTORY = True\n",
    "\n",
    "# ---------- Import pipeline ----------\n",
    "from Rectified import RectifiedFlowPipeline\n",
    "\n",
    "print(f\"[info] Loading pipeline for INT8 PTQ: {MODEL_ID} on {DEVICE} ({str(TORCH_DTYPE).split('.')[-1]})\")\n",
    "pipe_8bitQ = RectifiedFlowPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=TORCH_DTYPE,\n",
    "    use_safetensors=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "if DEVICE == \"cpu\":\n",
    "    torch.set_num_threads(max(1, os.cpu_count() - 1))\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def list_unet_dense_layers(unet: nn.Module) -> List[Tuple[str, nn.Module]]:\n",
    "    \"\"\"Return (name, module) for all Conv2d/Linear layers in the UNet.\"\"\"\n",
    "    layers = []\n",
    "    for name, mod in unet.named_modules():\n",
    "        if isinstance(mod, (nn.Conv2d, nn.Linear)):\n",
    "            layers.append((name, mod))\n",
    "    return layers\n",
    "\n",
    "def is_fragile(name: str, mod: nn.Module) -> bool:\n",
    "    \"\"\"\n",
    "    Skip list for first pass:\n",
    "      - First/last UNet convs (conv_in, conv_out)\n",
    "      - Time embedding layers\n",
    "      - Norm layers\n",
    "      - Attention output projection (often touchy initially)\n",
    "    \"\"\"\n",
    "    if \"conv_in\" in name or \"conv_out\" in name: return True\n",
    "    if \"time_embedding\" in name: return True\n",
    "    if isinstance(mod, (nn.LayerNorm, nn.GroupNorm)): return True\n",
    "    if \"attentions\" in name and (\"to_out\" in name or \"proj_out\" in name): return True\n",
    "    return False\n",
    "\n",
    "def get_parent_and_leaf(root: nn.Module, dotted: str):\n",
    "    parent = root\n",
    "    parts = dotted.split(\".\")\n",
    "    for p in parts[:-1]:\n",
    "        parent = getattr(parent, p)\n",
    "    return parent, parts[-1]\n",
    "\n",
    "# ---------- INT8 per-channel weights-only quantizer ----------\n",
    "# Symmetric per-OUT-channel:\n",
    "#   s_c = max(|W_c|) / 127\n",
    "#   q   = round(W / s_c), clipped to [-127, +127] (store in int8)\n",
    "#   Ŵ   = s_c * q   (dequant at forward)\n",
    "def per_channel_int8_quantize_weight(W: torch.Tensor):\n",
    "    oc = W.shape[0]\n",
    "    Wc = W.detach().clone().view(oc, -1)\n",
    "    scales = Wc.abs().max(dim=1).values / 127.0\n",
    "    scales = torch.clamp(scales, min=1e-8)\n",
    "    q = torch.round((Wc.T / scales).T)\n",
    "    q = torch.clamp(q, -127, 127).to(torch.int8)\n",
    "    return q, scales\n",
    "\n",
    "def dequantize_int8(q: torch.Tensor, scales: torch.Tensor, shape, to_dtype, to_device):\n",
    "    oc = shape[0]\n",
    "    Wf = (q.float().T * scales).T\n",
    "    return Wf.view(shape).to(dtype=to_dtype, device=to_device)\n",
    "\n",
    "class QConv2dINT8(nn.Module):\n",
    "    \"\"\"Drop-in Conv2d layer with INT8 weights (+ per-channel scales), dequantized each forward().\"\"\"\n",
    "    def __init__(self, conv: nn.Conv2d):\n",
    "        super().__init__()\n",
    "        # meta\n",
    "        self.stride, self.padding = conv.stride, conv.padding\n",
    "        self.dilation, self.groups = conv.dilation, conv.groups\n",
    "        self.bias_flag = conv.bias is not None\n",
    "        if self.bias_flag:\n",
    "            self.register_buffer(\"biasv\", conv.bias.data.detach().clone())\n",
    "        # quantize once\n",
    "        q, s = per_channel_int8_quantize_weight(conv.weight.data)\n",
    "        self.register_buffer(\"qweight\", q)\n",
    "        self.register_buffer(\"scales\", s)\n",
    "        # shape\n",
    "        self.out_channels = conv.out_channels\n",
    "        self.in_channels  = conv.in_channels\n",
    "        self.kernel_size  = conv.kernel_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        W = dequantize_int8(\n",
    "            self.qweight, self.scales,\n",
    "            (self.out_channels, self.in_channels, *self.kernel_size),\n",
    "            to_dtype=x.dtype, to_device=x.device\n",
    "        )\n",
    "        return F.conv2d(\n",
    "            x, W, bias=(self.biasv if self.bias_flag else None),\n",
    "            stride=self.stride, padding=self.padding,\n",
    "            dilation=self.dilation, groups=self.groups\n",
    "        )\n",
    "\n",
    "class QLinearINT8(nn.Module):\n",
    "    \"\"\"Drop-in Linear layer with INT8 weights (+ per-channel scales), dequantized each forward().\"\"\"\n",
    "    def __init__(self, lin: nn.Linear):\n",
    "        super().__init__()\n",
    "        # reuse conv quant path by treating (out,in) as (out,in,1,1)\n",
    "        w4d = lin.weight.data.unsqueeze(-1).unsqueeze(-1)\n",
    "        q, s = per_channel_int8_quantize_weight(w4d)\n",
    "        self.register_buffer(\"qweight\", q)\n",
    "        self.register_buffer(\"scales\", s)\n",
    "        self.bias_flag = lin.bias is not None\n",
    "        if self.bias_flag:\n",
    "            self.register_buffer(\"biasv\", lin.bias.data.detach().clone())\n",
    "        self.out_features = lin.out_features\n",
    "        self.in_features  = lin.in_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        W = dequantize_int8(\n",
    "            self.qweight, self.scales,\n",
    "            (self.out_features, self.in_features, 1, 1),\n",
    "            to_dtype=x.dtype, to_device=x.device\n",
    "        )\n",
    "        W = W.squeeze(-1).squeeze(-1)  # (out,in)\n",
    "        y = x @ W.t()\n",
    "        return y + (self.biasv if self.bias_flag else 0.0)\n",
    "\n",
    "# ---------- (optional) export UNet inventory before replacement ----------\n",
    "targets = list_unet_dense_layers(pipe_8bitQ.unet)\n",
    "\n",
    "if SAVE_INVENTORY:\n",
    "    os.makedirs(INV_DIR, exist_ok=True)\n",
    "    csv_path  = os.path.join(INV_DIR, \"unet_layers_int8.csv\")\n",
    "    json_path = os.path.join(INV_DIR, \"unet_layers_int8.json\")\n",
    "    rows, items = [], []\n",
    "    for idx, (name, mod) in enumerate(targets):\n",
    "        wshape = \"x\".join(map(str, list(mod.weight.shape)))\n",
    "        has_bias = (mod.bias is not None)\n",
    "        rows.append([idx, name, type(mod).__name__, wshape, has_bias])\n",
    "        items.append({\"idx\": idx, \"name\": name, \"type\": type(mod).__name__,\n",
    "                      \"weight_shape\": wshape, \"has_bias\": has_bias})\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"idx\",\"name\",\"type\",\"weight_shape\",\"has_bias\"])\n",
    "        writer.writerows(rows)\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump({\"layers\": items}, f, indent=2)\n",
    "    print(f\"[info] Saved UNet layer inventory (INT8 pass):\\n  • {csv_path}\\n  • {json_path}\")\n",
    "\n",
    "# ---------- Replace eligible layers with INT8 wrappers ----------\n",
    "num_quantized = 0\n",
    "for name, mod in targets:\n",
    "    if is_fragile(name, mod):\n",
    "        continue\n",
    "    parent, leaf = get_parent_and_leaf(pipe_8bitQ.unet, name)\n",
    "    if isinstance(mod, nn.Conv2d):\n",
    "        setattr(parent, leaf, QConv2dINT8(mod))\n",
    "        num_quantized += 1\n",
    "    elif isinstance(mod, nn.Linear):\n",
    "        setattr(parent, leaf, QLinearINT8(mod))\n",
    "        num_quantized += 1\n",
    "\n",
    "print(f\"[info] Quantized (weights-only INT8) layers in pipe_8bitQ: {num_quantized} (skipped fragile layers)\")\n",
    "\n",
    "# ---------- (Optional) quick generation sanity check ----------\n",
    "# OUT_DIR = \"Output_Images\"  # reuse if already defined elsewhere\n",
    "try:\n",
    "    OUT_DIR\n",
    "except NameError:\n",
    "    OUT_DIR = \"Output_Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "test_prompt = \"a cute dog\"\n",
    "WIDTH = HEIGHT = 512\n",
    "GUIDANCE = 0.0\n",
    "\n",
    "t0 = time.time()\n",
    "img = pipe_8bitQ(\n",
    "    prompt=test_prompt,\n",
    "    width=WIDTH, height=HEIGHT,\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=GUIDANCE\n",
    ").images[0]\n",
    "out_path = os.path.join(OUT_DIR, \"instaflow_w8_weights_only.png\")\n",
    "img.save(out_path)\n",
    "print(f\"[done] INT8 image saved: {out_path} | device={DEVICE} dtype={str(TORCH_DTYPE).split('.')[-1]} | {time.time()-t0:.2f}s\")\n",
    "\n",
    "display(Image.open(out_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_time(pipe, prompt, width=512, height=512, runs=3):\n",
    "    times = []\n",
    "    outs = []\n",
    "    torch.set_num_threads(max(1, os.cpu_count()-1))\n",
    "    for i in range(runs):\n",
    "        t0 = time.perf_counter()\n",
    "        img = pipe_8bitQ(prompt=prompt,\n",
    "                   width=width, height=height,\n",
    "                   num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "        dt = time.perf_counter() - t0\n",
    "        img.save(f\"Output_Images/8bit_instaflow_{width}x{height}_{i+1}.png\")\n",
    "        times.append(dt); outs.append(img)\n",
    "        print(f\"[{i+1}/{runs}] {width}x{height} in {dt:.2f}s -> instaflow_{width}x{height}_{i+1}.png\")\n",
    "    print(f\"\\nAvg time: {sum(times)/len(times):.2f}s | Min: {min(times):.2f}s | Max: {max(times):.2f}s\")\n",
    "    return times, outs\n",
    "\n",
    "# Example:\n",
    "_= generate_and_time(pipe, prompt=\"Cute cat, studio lighting\", width=512, height=512, runs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 bit Clip score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H = 512, 512\n",
    "gen_paths, gen_pils = [], []\n",
    "for i, p in enumerate(simple_prompts, 1):\n",
    "    im = pipe_8bitQ(prompt=p, width=W, height=H,\n",
    "              num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "    path = f\"Output_Images/8bit)qantized_clip_{i:02d}.png\"\n",
    "    im.save(path)\n",
    "    gen_paths.append(path)\n",
    "    gen_pils.append(Image.open(path).convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Convert PIL -> **uint8 tensors** [C,H,W] (no normalization, no /255)\n",
    "imgs_tensor_list = [TF.pil_to_tensor(img) for img in gen_pils]  # 0–255 uint8\n",
    "\n",
    "# 3) Build CLIPScore metric and compute per-image scores\n",
    "metric = CLIPScore(model_name_or_path=\"openai/clip-vit-large-patch14\").to(\"cpu\")\n",
    "\n",
    "scores = []\n",
    "with torch.no_grad():\n",
    "    for img_t, prompt in zip(imgs_tensor_list, simple_prompts):\n",
    "        # Pass single-item lists (image list, text list)\n",
    "        s = metric([img_t], [prompt]).item()\n",
    "        scores.append(s)\n",
    "\n",
    "avg = sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Display each image with its CLIPScore\n",
    "display(HTML(\"<h3>CLIPScore Results (Option A)</h3>\"))\n",
    "for idx, (p, path, s) in enumerate(zip(simple_prompts, gen_paths, scores), 1):\n",
    "    display(HTML(f\"<b>{idx:02d}. Prompt:</b> {p}<br><b>CLIPScore:</b> {s:.3f}\"))\n",
    "    display(Image.open(path))\n",
    "\n",
    "# 5) Save CSV file with per-image and average scores\n",
    "csv_path = \"8bit_quantized_clip_score.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"index\", \"image_path\", \"prompt\", \"clip_score\"])\n",
    "    for i, (path, p, s) in enumerate(zip(gen_paths, simple_prompts, scores), 1):\n",
    "        writer.writerow([i, path, p, f\"{s:.6f}\"])\n",
    "    writer.writerow([])\n",
    "    writer.writerow([\"avg\", \"\", \"\", f\"{avg:.6f}\"])\n",
    "\n",
    "print(f\"\\nSaved per-image CLIP scores and average to: {csv_path}\")\n",
    "print(f\"Average CLIPScore over {len(simple_prompts)} prompts: {avg:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Bit Quantizations Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [single-cell] InstaFlow 4-bit (weights-only) PTQ demo — load → quantize → generate\n",
    "# ------------------------------------------------------------------------------------\n",
    "# What this cell does:\n",
    "#   1) Loads the InstaFlow (Rectified Flow) text-to-image pipeline (0.9B) from Hugging Face\n",
    "#   2) Builds a compact inventory of UNet Conv/Linear layers\n",
    "#   3) Applies simple, per-output-channel **4-bit** (INT4) **weights-only** quantization\n",
    "#      to a safe subset of UNet layers (skips fragile spots like conv_in/conv_out/norms)\n",
    "#   4) Runs one-step generation and saves a quantized output image\n",
    "#\n",
    "# Notes:\n",
    "#   • This is a **quality exploration** path. We dequantize weights on-the-fly in forward(),\n",
    "#     so you won't see runtime speedups yet. It's the correct first step before moving to\n",
    "#     fused INT4 kernels (ONNX Runtime/TensorRT/CUTLASS) later.\n",
    "#   • The original downloaded weights are NOT modified on disk. We create a **separate**\n",
    "#     quantized pipeline instance in memory.\n",
    "#   • Works on CPU; GPU recommended for speed (set DEVICE = \"cuda\" if available).\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "import os, time, json, csv\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "MODEL_ID      = \"XCLiu/instaflow_0_9B_from_sd_1_5\"  # public HF model id\n",
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # \"cpu\" or \"cuda\"\n",
    "TORCH_DTYPE   = torch.float16 if (DEVICE == \"cuda\") else torch.float32\n",
    "PROMPT        = \"two cute cats playing with yarn, soft lighting, photorealistic\"\n",
    "WIDTH = HEIGHT = 512\n",
    "GUIDANCE      = 0.0   # InstaFlow is one-step; CFG>1.0 does extra work. 0.0 keeps it simple.\n",
    "OUT_DIR       = \"Output_Images\"\n",
    "INV_DIR       = \"Inventory\"\n",
    "SAVE_INVENTORY= True  # saves a CSV/JSON inventory of quantized layers\n",
    "\n",
    "# ---------- 0) Load baseline pipeline ----------\n",
    "# Expect to run inside the InstaFlow repo's `code/` folder so this import works:\n",
    "from Rectified import RectifiedFlowPipeline\n",
    "\n",
    "print(f\"[info] Loading pipeline {MODEL_ID} on {DEVICE} as {str(TORCH_DTYPE).split('.')[-1]} ...\")\n",
    "pipe_base = RectifiedFlowPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=TORCH_DTYPE,\n",
    "    use_safetensors=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "if DEVICE == \"cpu\":\n",
    "    torch.set_num_threads(max(1, os.cpu_count() - 1))\n",
    "\n",
    "# ---------- 1) UNet inventory helpers ----------\n",
    "def list_unet_dense_layers(unet: nn.Module) -> List[Tuple[str, nn.Module]]:\n",
    "    \"\"\"Return (name, module) for all Conv2d/Linear layers in the UNet.\"\"\"\n",
    "    layers = []\n",
    "    for name, mod in unet.named_modules():\n",
    "        if isinstance(mod, (nn.Conv2d, nn.Linear)):\n",
    "            layers.append((name, mod))\n",
    "    return layers\n",
    "\n",
    "def is_fragile(name: str, mod: nn.Module) -> bool:\n",
    "    \"\"\"\n",
    "    First quantization pass skip-list:\n",
    "      - First/last UNet convs (conv_in, conv_out)\n",
    "      - Time embedding layers\n",
    "      - Norms (handled elsewhere; keep in float)\n",
    "      - Attention output projection (often touchy on first pass)\n",
    "    \"\"\"\n",
    "    if \"conv_in\" in name or \"conv_out\" in name: return True\n",
    "    if \"time_embedding\" in name: return True\n",
    "    if isinstance(mod, (nn.LayerNorm, nn.GroupNorm)): return True\n",
    "    if \"attentions\" in name and (\"to_out\" in name or \"proj_out\" in name): return True\n",
    "    return False\n",
    "\n",
    "# ---------- 2) Simple 4-bit quantizer (weights-only, per-out-channel) ----------\n",
    "# We quantize each OUT channel independently: s_c = max(|W_c|)/7; q = round(W/s_c) in [-8, 7].\n",
    "# We store q as int8 but values live in 4-bit domain (two's complement nibble).\n",
    "# In forward(), we dequantize: W_hat = s_c * q  (computed to match input dtype/device).\n",
    "def per_channel_int4_quantize_weight(W: torch.Tensor):\n",
    "    \"\"\"\n",
    "    W shape:\n",
    "      • Conv2d: (out_c, in_c, kH, kW)\n",
    "      • Linear: (out_c, in_c)\n",
    "    Returns:\n",
    "      q (int8 tensor holding 4-bit domain values), scales (float per out-channel)\n",
    "    \"\"\"\n",
    "    oc = W.shape[0]\n",
    "    Wc = W.detach().clone().view(oc, -1)               # flatten per OUT channel\n",
    "    scales = Wc.abs().max(dim=1).values / 7.0          # s_c = max|W_c| / 7\n",
    "    scales = torch.clamp(scales, min=1e-8)             # avoid div-by-zero\n",
    "    q = torch.round((Wc.T / scales).T)\n",
    "    q = torch.clamp(q, -8, 7).to(torch.int8)           # 4-bit domain values stored in int8\n",
    "    return q, scales\n",
    "\n",
    "def dequantize_int4(q: torch.Tensor, scales: torch.Tensor, shape, to_dtype, to_device):\n",
    "    oc = shape[0]\n",
    "    Wf = (q.float().T * scales).T                      # W_hat = s * q\n",
    "    return Wf.view(shape).to(dtype=to_dtype, device=to_device)\n",
    "\n",
    "class QConv2dINT4(nn.Module):\n",
    "    \"\"\"Drop-in Conv2d that holds INT4 weights (+ per-channel scales) and dequants per forward().\"\"\"\n",
    "    def __init__(self, conv: nn.Conv2d):\n",
    "        super().__init__()\n",
    "        # save Conv2d meta\n",
    "        self.stride, self.padding = conv.stride, conv.padding\n",
    "        self.dilation, self.groups = conv.dilation, conv.groups\n",
    "        self.bias_flag = conv.bias is not None\n",
    "        if self.bias_flag:\n",
    "            self.register_buffer(\"biasv\", conv.bias.data.detach().clone())\n",
    "        # quantize weights once\n",
    "        q, s = per_channel_int4_quantize_weight(conv.weight.data)\n",
    "        self.register_buffer(\"qweight\", q)\n",
    "        self.register_buffer(\"scales\", s)\n",
    "        # shapes\n",
    "        self.out_channels = conv.out_channels\n",
    "        self.in_channels  = conv.in_channels\n",
    "        self.kernel_size  = conv.kernel_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        W = dequantize_int4(\n",
    "            self.qweight, self.scales,\n",
    "            (self.out_channels, self.in_channels, *self.kernel_size),\n",
    "            to_dtype=x.dtype, to_device=x.device\n",
    "        )\n",
    "        return F.conv2d(\n",
    "            x, W, bias=(self.biasv if self.bias_flag else None),\n",
    "            stride=self.stride, padding=self.padding,\n",
    "            dilation=self.dilation, groups=self.groups\n",
    "        )\n",
    "\n",
    "class QLinearINT4(nn.Module):\n",
    "    \"\"\"Drop-in Linear with INT4 weights (+ per-channel scales), dequant per forward().\"\"\"\n",
    "    def __init__(self, lin: nn.Linear):\n",
    "        super().__init__()\n",
    "        # treat (out,in) as (out,in,1,1) for shared quant code\n",
    "        w4d = lin.weight.data.unsqueeze(-1).unsqueeze(-1)\n",
    "        q, s = per_channel_int4_quantize_weight(w4d)\n",
    "        self.register_buffer(\"qweight\", q)\n",
    "        self.register_buffer(\"scales\", s)\n",
    "        self.bias_flag = lin.bias is not None\n",
    "        if self.bias_flag:\n",
    "            self.register_buffer(\"biasv\", lin.bias.data.detach().clone())\n",
    "        self.out_features = lin.out_features\n",
    "        self.in_features  = lin.in_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        W = dequantize_int4(self.qweight, self.scales,\n",
    "                            (self.out_features, self.in_features, 1, 1),\n",
    "                            to_dtype=x.dtype, to_device=x.device)\n",
    "        W = W.squeeze(-1).squeeze(-1)  # (out,in)\n",
    "        y = x @ W.t()\n",
    "        return y + (self.biasv if self.bias_flag else 0.0)\n",
    "\n",
    "# ---------- 3) Build a separate quantized pipeline (keeps baseline intact) ----------\n",
    "print(\"[info] Creating a separate quantized pipeline instance ...\")\n",
    "pipe_q = RectifiedFlowPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=TORCH_DTYPE,\n",
    "    use_safetensors=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "# identify target layers in UNet\n",
    "targets = list_unet_dense_layers(pipe_q.unet)\n",
    "\n",
    "# optional: export inventory before replacement\n",
    "if SAVE_INVENTORY:\n",
    "    os.makedirs(INV_DIR, exist_ok=True)\n",
    "    csv_path  = os.path.join(INV_DIR, \"unet_layers.csv\")\n",
    "    json_path = os.path.join(INV_DIR, \"unet_layers.json\")\n",
    "    rows, items = [], []\n",
    "    for idx, (name, mod) in enumerate(targets):\n",
    "        wshape = \"x\".join(map(str, list(mod.weight.shape)))\n",
    "        has_bias = (mod.bias is not None)\n",
    "        rows.append([idx, name, type(mod).__name__, wshape, has_bias])\n",
    "        items.append({\"idx\": idx, \"name\": name, \"type\": type(mod).__name__,\n",
    "                      \"weight_shape\": wshape, \"has_bias\": has_bias})\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f); writer.writerow([\"idx\",\"name\",\"type\",\"weight_shape\",\"has_bias\"]); writer.writerows(rows)\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump({\"layers\": items}, f, indent=2)\n",
    "    print(f\"[info] Saved UNet layer inventory:\\n  • {csv_path}\\n  • {json_path}\")\n",
    "\n",
    "# replacement helper: navigate to parent of a dotted module path\n",
    "def get_parent_and_leaf(root: nn.Module, dotted: str):\n",
    "    parent = root\n",
    "    parts = dotted.split(\".\")\n",
    "    for p in parts[:-1]:\n",
    "        parent = getattr(parent, p)\n",
    "    return parent, parts[-1]\n",
    "\n",
    "# replace eligible layers with INT4 wrappers\n",
    "num_quantized = 0\n",
    "for name, mod in targets:\n",
    "    if is_fragile(name, mod):\n",
    "        continue\n",
    "    parent, leaf = get_parent_and_leaf(pipe_q.unet, name)\n",
    "    if isinstance(mod, nn.Conv2d):\n",
    "        setattr(parent, leaf, QConv2dINT4(mod))\n",
    "        num_quantized += 1\n",
    "    elif isinstance(mod, nn.Linear):\n",
    "        setattr(parent, leaf, QLinearINT4(mod))\n",
    "        num_quantized += 1\n",
    "\n",
    "print(f\"[info] Quantized (weights-only INT4) layers: {num_quantized} (skipped fragile layers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4) Generate one image with the quantized pipeline ----------\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "PROMPTQ = \"a cute cat\"\n",
    "t0 = time.time()\n",
    "img = pipe_q(\n",
    "    prompt=PROMPTQ,\n",
    "    width=WIDTH, height=HEIGHT,\n",
    "    num_inference_steps=1,   # InstaFlow is one-step\n",
    "    guidance_scale=GUIDANCE\n",
    ").images[0]\n",
    "out_path = os.path.join(OUT_DIR, \"instaflow_w4_weights_only.png\")\n",
    "img.save(out_path)\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "print(f\"[done] Saved: {out_path}  | device={DEVICE} dtype={str(TORCH_DTYPE).split('.')[-1]} | {elapsed:.2f}s\")\n",
    "display(Image.open(out_path))\n",
    "\n",
    "\n",
    "#This si the best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP score for quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H = 512, 512\n",
    "gen_paths, gen_pils = [], []\n",
    "for i, p in enumerate(simple_prompts, 1):\n",
    "    im = pipe_q(prompt=p, width=W, height=H,\n",
    "              num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "    path = f\"Output_Images/qantized_clip_{i:02d}.png\"\n",
    "    im.save(path)\n",
    "    gen_paths.append(path)\n",
    "    gen_pils.append(Image.open(path).convert(\"RGB\"))\n",
    "\n",
    "# 2) Convert PIL -> **uint8 tensors** [C,H,W] (no normalization, no /255)\n",
    "imgs_tensor_list = [TF.pil_to_tensor(img) for img in gen_pils]  # 0–255 uint8\n",
    "\n",
    "# 3) Build CLIPScore metric and compute per-image scores\n",
    "metric = CLIPScore(model_name_or_path=\"openai/clip-vit-large-patch14\").to(\"cpu\")\n",
    "\n",
    "scores = []\n",
    "with torch.no_grad():\n",
    "    for img_t, prompt in zip(imgs_tensor_list, simple_prompts):\n",
    "        # Pass single-item lists (image list, text list)\n",
    "        s = metric([img_t], [prompt]).item()\n",
    "        scores.append(s)\n",
    "\n",
    "avg = sum(scores) / len(scores)\n",
    "\n",
    "# 4) Display each image with its CLIPScore\n",
    "display(HTML(\"<h3>CLIPScore Results (Option A)</h3>\"))\n",
    "for idx, (p, path, s) in enumerate(zip(simple_prompts, gen_paths, scores), 1):\n",
    "    display(HTML(f\"<b>{idx:02d}. Prompt:</b> {p}<br><b>CLIPScore:</b> {s:.3f}\"))\n",
    "    display(Image.open(path))\n",
    "\n",
    "# 5) Save CSV file with per-image and average scores\n",
    "csv_path = \"quantized_clip_score.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"index\", \"image_path\", \"prompt\", \"clip_score\"])\n",
    "    for i, (path, p, s) in enumerate(zip(gen_paths, simple_prompts, scores), 1):\n",
    "        writer.writerow([i, path, p, f\"{s:.6f}\"])\n",
    "    writer.writerow([])\n",
    "    writer.writerow([\"avg\", \"\", \"\", f\"{avg:.6f}\"])\n",
    "\n",
    "print(f\"\\nSaved per-image CLIP scores and average to: {csv_path}\")\n",
    "print(f\"Average CLIPScore over {len(simple_prompts)} prompts: {avg:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Convert PIL -> **uint8 tensors** [C,H,W] (no normalization, no /255)\n",
    "imgs_tensor_list = [TF.pil_to_tensor(img) for img in gen_pils]  # 0–255 uint8\n",
    "\n",
    "# 3) Build CLIPScore metric and compute per-image scores\n",
    "metric = CLIPScore(model_name_or_path=\"openai/clip-vit-large-patch14\").to(\"cpu\")\n",
    "\n",
    "scores = []\n",
    "with torch.no_grad():\n",
    "    for img_t, prompt in zip(imgs_tensor_list, simple_prompts):\n",
    "        # Pass single-item lists (image list, text list)\n",
    "        s = metric([img_t], [prompt]).item()\n",
    "        scores.append(s)\n",
    "\n",
    "avg = sum(scores) / len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Display each image with its CLIPScore\n",
    "display(HTML(\"<h3>CLIPScore Results (Option A)</h3>\"))\n",
    "for idx, (p, path, s) in enumerate(zip(simple_prompts, gen_paths, scores), 1):\n",
    "    display(HTML(f\"<b>{idx:02d}. Prompt:</b> {p}<br><b>CLIPScore:</b> {s:.3f}\"))\n",
    "    display(Image.open(path))\n",
    "\n",
    "# 5) Save CSV file with per-image and average scores\n",
    "csv_path = \"quantized_clip_score.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"index\", \"image_path\", \"prompt\", \"clip_score\"])\n",
    "    for i, (path, p, s) in enumerate(zip(gen_paths, simple_prompts, scores), 1):\n",
    "        writer.writerow([i, path, p, f\"{s:.6f}\"])\n",
    "    writer.writerow([])\n",
    "    writer.writerow([\"avg\", \"\", \"\", f\"{avg:.6f}\"])\n",
    "\n",
    "print(f\"\\nSaved per-image CLIP scores and average to: {csv_path}\")\n",
    "print(f\"Average CLIPScore over {len(simple_prompts)} prompts: {avg:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Build And checking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Missing packages: torch, onnxruntime\n",
      "Install them (CPU-only):\n",
      "  pip install --upgrade torch --index-url https://download.pytorch.org/whl/cpu\n",
      "  pip install onnx onnxruntime\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# %% [single-cell] Quick ONNX Runtime CPU sanity check + real-time diff vs PyTorch\n",
    "import sys, time, statistics, math\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------- dependency probe -----------------------------\n",
    "missing = []\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    missing.append(\"torch\")\n",
    "try:\n",
    "    import onnx\n",
    "except Exception:\n",
    "    missing.append(\"onnx\")\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "except Exception:\n",
    "    missing.append(\"onnxruntime\")\n",
    "\n",
    "if missing:\n",
    "    print(\"❌ Missing packages:\", \", \".join(missing))\n",
    "    print(\"Install them (CPU-only):\")\n",
    "    print(\"  pip install --upgrade torch --index-url https://download.pytorch.org/whl/cpu\")\n",
    "    print(\"  pip install onnx onnxruntime\")\n",
    "    raise SystemExit\n",
    "\n",
    "print(f\"✅ torch {torch.__version__} | onnx {onnx.__version__} | onnxruntime {ort.__version__}\")\n",
    "\n",
    "# ----------------------------- tiny test model ------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TinyBlock(nn.Module):\n",
    "    def __init__(self, C_in=3, C_mid=16, C_out=32, H=64, W=64):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(C_in, C_mid, kernel_size=3, padding=1)\n",
    "        self.lin  = nn.Linear(C_mid*H*W, C_out)\n",
    "        self.H, self.W = H, W\n",
    "    def forward(self, x):\n",
    "        # x: (N, C_in, H, W)\n",
    "        y = self.conv(x)\n",
    "        y = F.relu(y)\n",
    "        y = y.reshape(y.shape[0], -1)  # flatten\n",
    "        y = self.lin(y)\n",
    "        return y\n",
    "\n",
    "# fix shapes to keep things simple and comparable\n",
    "B, C_in, H, W = 1, 3, 64, 64\n",
    "model = TinyBlock(C_in=C_in, C_mid=16, C_out=32, H=H, W=W).cpu().eval()\n",
    "dummy = torch.randn(B, C_in, H, W, dtype=torch.float32)\n",
    "\n",
    "# ----------------------------- export to ONNX -------------------------------\n",
    "onnx_path = Path(\"tinyblock.onnx\").absolute()\n",
    "OPSET = 17  # set to 21 if you plan to use newest quantizers later\n",
    "torch.onnx.export(\n",
    "    model, dummy, onnx_path.as_posix(),\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=OPSET,\n",
    "    do_constant_folding=True,\n",
    "    dynamic_axes=None,  # fixed-shape for this quick test\n",
    ")\n",
    "print(f\"📦 Exported ONNX to: {onnx_path}\")\n",
    "\n",
    "# basic model load check\n",
    "_ = onnx.load(onnx_path.as_posix())\n",
    "onnx.checker.check_model(_)\n",
    "print(\"✅ ONNX graph loads & passes checker\")\n",
    "\n",
    "# ----------------------------- ORT session (CPU) ----------------------------\n",
    "sess = ort.InferenceSession(\n",
    "    onnx_path.as_posix(),\n",
    "    providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "print(\"🧠 ORT providers:\", sess.get_providers())\n",
    "\n",
    "# helper to run ORT\n",
    "import numpy as np\n",
    "def run_ort(x_torch: torch.Tensor):\n",
    "    io_binding = None\n",
    "    x_np = x_torch.detach().cpu().numpy()\n",
    "    ort_inputs = {sess.get_inputs()[0].name: x_np}\n",
    "    out = sess.run(None, ort_inputs)[0]\n",
    "    return out\n",
    "\n",
    "# ----------------------------- correctness check ----------------------------\n",
    "with torch.no_grad():\n",
    "    ref = model(dummy).cpu().numpy()\n",
    "    ort_out = run_ort(dummy)\n",
    "\n",
    "# numeric diffs\n",
    "abs_diff = np.abs(ref - ort_out)\n",
    "max_abs = float(abs_diff.max())\n",
    "mean_abs = float(abs_diff.mean())\n",
    "print(f\"🔍 Numeric diff vs PyTorch: max_abs={max_abs:.3e}, mean_abs={mean_abs:.3e}\")\n",
    "\n",
    "# ----------------------------- timing (real-time diff) ----------------------\n",
    "def bench_pytorch(model, x, warmup=10, iters=100):\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(x)\n",
    "        for _ in range(iters):\n",
    "            t0 = time.perf_counter()\n",
    "            _ = model(x)\n",
    "            times.append((time.perf_counter() - t0)*1000.0)\n",
    "    return times\n",
    "\n",
    "def bench_onnx(sess, x, warmup=10, iters=100):\n",
    "    times = []\n",
    "    _ = run_ort(x)  # quick touch\n",
    "    for _ in range(warmup):\n",
    "        _ = run_ort(x)\n",
    "    for _ in range(iters):\n",
    "        t0 = time.perf_counter()\n",
    "        _ = run_ort(x)\n",
    "        times.append((time.perf_counter() - t0)*1000.0)\n",
    "    return times\n",
    "\n",
    "print(\"\\n⏱️  Benchmarking on CPU…\")\n",
    "pt_times  = bench_pytorch(model, dummy, warmup=10, iters=100)\n",
    "ort_times = bench_onnx(sess, dummy,  warmup=10, iters=100)\n",
    "\n",
    "def stats(xs):\n",
    "    return {\n",
    "        \"avg_ms\": statistics.mean(xs),\n",
    "        \"p50_ms\": statistics.median(xs),\n",
    "        \"p95_ms\": statistics.quantiles(xs, n=20)[18],  # approx 95th\n",
    "        \"min_ms\": min(xs),\n",
    "        \"max_ms\": max(xs),\n",
    "        \"n\": len(xs),\n",
    "    }\n",
    "\n",
    "pt_s  = stats(pt_times)\n",
    "ort_s = stats(ort_times)\n",
    "\n",
    "print(f\"PyTorch  CPU  -> avg {pt_s['avg_ms']:.3f} ms | p50 {pt_s['p50_ms']:.3f} | p95 {pt_s['p95_ms']:.3f} | min {pt_s['min_ms']:.3f}\")\n",
    "print(f\"ONNX RT  CPU  -> avg {ort_s['avg_ms']:.3f} ms | p50 {ort_s['p50_ms']:.3f} | p95 {ort_s['p95_ms']:.3f} | min {ort_s['min_ms']:.3f}\")\n",
    "\n",
    "speedup = pt_s[\"avg_ms\"] / ort_s[\"avg_ms\"] if ort_s[\"avg_ms\"] > 0 else float(\"inf\")\n",
    "print(f\"\\n⚖️  Reported numeric diff (max/mean abs): {max_abs:.3e} / {mean_abs:.3e}\")\n",
    "print(f\"🚀 Relative speedup (PyTorch avg / ORT avg): {speedup:.2f}×\")\n",
    "\n",
    "print(\"\\nDone. If ORT runs and numeric diffs are tiny, your ONNX toolchain is good to go.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe: c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\Scripts\\python.exe\n",
      "Python ver: 3.10.0 | OS: Windows-10-10.0.26200-SP0\n",
      "> c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\Scripts\\python.exe -m pip install --upgrade --force-reinstall pip setuptools wheel\n",
      "> c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\Scripts\\python.exe -m pip install --upgrade --force-reinstall onnx\n",
      "> c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\Scripts\\python.exe -m pip install --upgrade --force-reinstall onnxruntime\n",
      "> c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\Scripts\\python.exe -m pip install --upgrade --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
      "✅ import torch ok — version: 2.9.0+cpu\n",
      "✅ import onnx ok — version: 1.19.1\n",
      "❌ import onnxruntime failed: DLL load failed while importing onnxruntime_pybind11_state: A dynamic link library (DLL) initialization routine failed.\n",
      "\n",
      "Troubleshooting tips:\n",
      "• If you see multiple Python installs, ensure your Jupyter kernel uses this Python exe shown above.\n",
      "• In Jupyter, always prefer `%pip install ...` (or the pattern we used: `sys.executable -m pip ...`).\n",
      "• If you previously installed `onnxruntime-gpu`, uninstall it: `pip uninstall -y onnxruntime-gpu` and keep `onnxruntime`.\n"
     ]
    }
   ],
   "source": [
    "# %% Fix & verify torch / onnx / onnxruntime inside THIS Jupyter kernel\n",
    "\n",
    "import sys, subprocess, pkgutil, platform\n",
    "\n",
    "print(\"Python exe:\", sys.executable)\n",
    "print(\"Python ver:\", sys.version.split()[0], \"| OS:\", platform.platform())\n",
    "\n",
    "def pip_install(*args):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", *args]\n",
    "    print(\">\", \" \".join(cmd))\n",
    "    return subprocess.check_call(cmd)\n",
    "\n",
    "# 1) Show where pip would install and what pip is being used\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-V\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"list\"])\n",
    "\n",
    "# 2) (Re)install CPU wheels into THIS kernel's environment\n",
    "#    - Force reinstall to avoid path confusion\n",
    "#    - Torch CPU: use the official CPU wheel index (works on Windows/Linux/macOS-Intel)\n",
    "pip_install(\"--upgrade\", \"--force-reinstall\", \"pip\", \"setuptools\", \"wheel\")\n",
    "pip_install(\"--upgrade\", \"--force-reinstall\", \"onnx\")\n",
    "pip_install(\"--upgrade\", \"--force-reinstall\", \"onnxruntime\")   # CPU package\n",
    "try:\n",
    "    pip_install(\"--upgrade\", \"--force-reinstall\",\n",
    "                \"torch\", \"torchvision\", \"torchaudio\",\n",
    "                \"--index-url\", \"https://download.pytorch.org/whl/cpu\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"PyTorch CPU install via index-url failed; trying default PyPI (still CPU-only).\")\n",
    "    pip_install(\"--upgrade\", \"--force-reinstall\", \"torch\", \"torchvision\", \"torchaudio\")\n",
    "\n",
    "# 3) Sanity-imports in THIS kernel\n",
    "errs = []\n",
    "for mod in (\"torch\", \"onnx\", \"onnxruntime\"):\n",
    "    try:\n",
    "        __import__(mod)\n",
    "        m = sys.modules[mod]\n",
    "        print(f\"✅ import {mod} ok — version:\", getattr(m, \"__version__\", \"n/a\"))\n",
    "    except Exception as e:\n",
    "        errs.append((mod, repr(e)))\n",
    "        print(f\"❌ import {mod} failed:\", e)\n",
    "\n",
    "# 4) Extra checks & tips\n",
    "if errs:\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"• If you see multiple Python installs, ensure your Jupyter kernel uses this Python exe shown above.\")\n",
    "    print(\"• In Jupyter, always prefer `%pip install ...` (or the pattern we used: `sys.executable -m pip ...`).\")\n",
    "    print(\"• If you previously installed `onnxruntime-gpu`, uninstall it: `pip uninstall -y onnxruntime-gpu` and keep `onnxruntime`.\")\n",
    "else:\n",
    "    print(\"\\nAll good. You can proceed with ONNX Runtime on CPU in this notebook.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python  : c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\Scripts\\python.exe\n",
      "Version : 3.10.0 | OS: Windows-10-10.0.26200-SP0\n",
      "> c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\Scripts\\python.exe -m pip uninstall -y onnxruntime-gpu\n",
      "> c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\Scripts\\python.exe -m pip uninstall -y onnxruntime-directml\n",
      "> c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\Scripts\\python.exe -m pip uninstall -y onnxruntime-extensions\n",
      "> c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\Scripts\\python.exe -m pip install --upgrade --force-reinstall numpy==1.26.4\n",
      "> c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\Scripts\\python.exe -m pip install --upgrade --force-reinstall onnxruntime==1.17.3\n",
      "> c:\\Users\\Akshat Kumar\\OneDrive - Indian Institute of Technology Bombay\\Desktop\\IITB Notes\\7th_Sem\\IE 643\\PROJECT\\.instaflow310\\Scripts\\python.exe -m pip install --upgrade --force-reinstall onnx==1.15.0\n",
      "\n",
      "✅ Packages aligned. IMPORTANT: Restart the Jupyter kernel now (Kernel → Restart).\n",
      "After restart, run the sanity cell I’ll give you next.\n"
     ]
    }
   ],
   "source": [
    "# %% Windows ORT + NumPy ABI fix (run once in THIS kernel)\n",
    "import sys, subprocess, platform\n",
    "\n",
    "def pip(*args):\n",
    "    print(\">\", sys.executable, \"-m\", \"pip\", *args)\n",
    "    return subprocess.check_call([sys.executable, \"-m\", \"pip\", *args])\n",
    "\n",
    "print(\"Python  :\", sys.executable)\n",
    "print(\"Version :\", sys.version.split()[0], \"| OS:\", platform.platform())\n",
    "\n",
    "# 0) Remove GPU/DML variants (if any)\n",
    "for pkg in (\"onnxruntime-gpu\", \"onnxruntime-directml\"):\n",
    "    try:\n",
    "        pip(\"uninstall\", \"-y\", pkg)\n",
    "    except subprocess.CalledProcessError:\n",
    "        pass\n",
    "\n",
    "# 1) Eject onnxruntime-extensions FOR NOW (it auto-imports ORT and can re-trigger the mismatch)\n",
    "try:\n",
    "    pip(\"uninstall\", \"-y\", \"onnxruntime-extensions\")\n",
    "except subprocess.CalledProcessError:\n",
    "    pass\n",
    "\n",
    "# 2) Force NumPy to a pre-2.0 ABI and reinstall ORT CPU against it\n",
    "pip(\"install\", \"--upgrade\", \"--force-reinstall\", \"numpy==1.26.4\")\n",
    "pip(\"install\", \"--upgrade\", \"--force-reinstall\", \"onnxruntime==1.17.3\")\n",
    "\n",
    "# 3) If ONNX itself complains later, we’ll re-pin it; preemptively align it now:\n",
    "pip(\"install\", \"--upgrade\", \"--force-reinstall\", \"onnx==1.15.0\")\n",
    "\n",
    "print(\"\\n✅ Packages aligned. IMPORTANT: Restart the Jupyter kernel now (Kernel → Restart).\")\n",
    "print(\"After restart, run the sanity cell I’ll give you next.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".instaflow310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
